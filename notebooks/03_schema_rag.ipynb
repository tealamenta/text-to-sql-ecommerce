{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Schema RAG : Améliorer avec le contexte\n",
    "\n",
    "**Objectif** : Améliorer l'accuracy en donnant plus de contexte au LLM\n",
    "\n",
    "**Améliorations** :\n",
    "1. Schéma enrichi avec exemples de données\n",
    "2. Règles SQLite spécifiques\n",
    "3. Instructions plus strictes\n",
    "\n",
    "**Baseline** : 25% EX → **Objectif : 50%+**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Database connectée\n",
      "✓ 12 questions chargées\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path('../data')\n",
    "DB_PATH = DATA_DIR / 'database' / 'ecommerce.db'\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "\n",
    "# Charger test set\n",
    "with open(DATA_DIR / 'results' / 'test_questions.json', 'r') as f:\n",
    "    TEST_QUESTIONS = json.load(f)\n",
    "\n",
    "print(f'✓ Database connectée')\n",
    "print(f'✓ {len(TEST_QUESTIONS)} questions chargées')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Schéma enrichi avec exemples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- customers (99441 rows)\n",
      "CREATE TABLE customers (\n",
      "  customer_id TEXT  -- ex: '06b8999e2fba1a1fbc88172c00ba8, '18955e83d337fd6b2def6b18a428a\n",
      "  customer_city TEXT  -- ex: 'franca', 'sao bernardo do campo'\n",
      "  customer_state TEXT  -- ex: 'SP', 'SP'\n",
      ");\n",
      "\n",
      "-- products (32951 rows)\n",
      "CREATE TABLE products (\n",
      "  product_id TEXT  -- ex: '1e9e8ef04dbcff4541ed26657ea51, '3aa071139cb16b67ca9e5dea641aa\n",
      "  product_category TEXT  -- ex: 'perfumaria', 'artes'\n",
      "  product_weight_g REAL  -- ex: 225.0, 1000.0\n",
      "  product_length_cm REAL  -- ex: 16.0, 30.0\n",
      "  product_height_cm REAL  -- ex: 10.0, 18.0\n",
      "  product_width_cm REAL  -- ex: 14.0, 20.0\n",
      ");\n",
      "\n",
      "-- orders (99441 rows)\n",
      "CREATE TABLE orders (\n",
      "  order_id TEXT  -- ex: 'e481f51cbdc54678b7cc49136f2d6, '53cdb2fc8bc7dce0b6741e2150273\n",
      "  customer_id TEXT  -- ex: '9ef432eb6251297304e76186b10a9, 'b0830fb4747a6c6d20dea0b8c802d\n",
      "  order_status TEXT  -- ex: 'delivered', 'delivered'\n",
      "  order_purchase_timestamp TEXT  -- ex: '2017-10-02 10:56:33', '2018-07-24 20:41:37'\n",
      "  order_delivered_timestamp TEXT  -- ex: '2017-10-10 21:25:13', '2018-08-07 15:27:45'\n",
      ");\n",
      "\n",
      "-- order_items (112650 rows)\n",
      "CREATE TABLE order_items (\n",
      "  order_id TEXT  -- ex: '00010242fe8c5a6d1ba2dd792cb16, '00018f77f2f0320c557190d7a144b\n",
      "  order_item_id INTEGER  -- ex: 1, 1\n",
      "  product_id TEXT  -- ex: '4244733e06e7ecb4970a6e2683c13, 'e5f2d52b802189ee658865ca93d83\n",
      "  price REAL  -- ex: 58.9, 239.9\n",
      "  freight_value REAL  -- ex: 13.29, 19.93\n",
      ");\n",
      "\n",
      "-- payments (103886 rows)\n",
      "CREATE TABLE payments (\n",
      "  order_id TEXT  -- ex: 'b81ef226f3fe1789b1e8b2acac839, 'a9810da82917af2d9aefd1278f1dc\n",
      "  payment_sequential INTEGER  -- ex: 1, 1\n",
      "  payment_type TEXT  -- ex: 'credit_card', 'credit_card'\n",
      "  payment_installments INTEGER  -- ex: 8, 1\n",
      "  payment_value REAL  -- ex: 99.33, 24.39\n",
      ");\n",
      "\n",
      "-- reviews (99224 rows)\n",
      "CREATE TABLE reviews (\n",
      "  review_id TEXT  -- ex: '7bc2406110b926393aa56f80a40eb, '80e641a11e56f04c1ad469d5645fd\n",
      "  order_id TEXT  -- ex: '73fc7af87114b39712e6da79b0a37, 'a548910a1c6147796b98fdf73dbeb\n",
      "  review_score INTEGER  -- ex: 4, 5\n",
      "  review_comment_title TEXT  -- ex: \n",
      "  review_comment_message TEXT  -- ex: \n",
      ");\n"
     ]
    }
   ],
   "source": [
    "def get_enriched_schema(conn) -> str:\n",
    "    \"\"\"Créer un schéma enrichi avec exemples de données.\"\"\"\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = [t[0] for t in cursor.fetchall()]\n",
    "    \n",
    "    schema_parts = []\n",
    "    \n",
    "    for table in tables:\n",
    "        # Get columns\n",
    "        cursor.execute(f\"PRAGMA table_info({table})\")\n",
    "        columns = cursor.fetchall()\n",
    "        \n",
    "        # Get sample data\n",
    "        sample = pd.read_sql(f\"SELECT * FROM {table} LIMIT 3\", conn)\n",
    "        \n",
    "        # Get row count\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "        count = cursor.fetchone()[0]\n",
    "        \n",
    "        # Build schema\n",
    "        cols_str = []\n",
    "        for col in columns:\n",
    "            col_name = col[1]\n",
    "            col_type = col[2] or 'TEXT'\n",
    "            # Get sample values\n",
    "            sample_vals = sample[col_name].dropna().head(2).tolist()\n",
    "            sample_str = ', '.join([repr(v)[:30] for v in sample_vals])\n",
    "            cols_str.append(f\"  {col_name} {col_type}  -- ex: {sample_str}\")\n",
    "        \n",
    "        table_schema = f\"\"\"-- {table} ({count} rows)\n",
    "CREATE TABLE {table} (\n",
    "{chr(10).join(cols_str)}\n",
    ");\"\"\"\n",
    "        schema_parts.append(table_schema)\n",
    "    \n",
    "    return \"\\n\\n\".join(schema_parts)\n",
    "\n",
    "ENRICHED_SCHEMA = get_enriched_schema(conn)\n",
    "print(ENRICHED_SCHEMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prompt amélioré avec règles SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt template créé\n",
      "Longueur: 604 chars\n"
     ]
    }
   ],
   "source": [
    "IMPROVED_PROMPT = '''You are a SQL expert. Generate SQLite queries for an e-commerce database.\n",
    "\n",
    "DATABASE SCHEMA:\n",
    "{schema}\n",
    "\n",
    "RELATIONSHIPS:\n",
    "- customers.customer_id -> orders.customer_id\n",
    "- orders.order_id -> order_items.order_id\n",
    "- orders.order_id -> payments.order_id\n",
    "- orders.order_id -> reviews.order_id\n",
    "- products.product_id -> order_items.product_id\n",
    "\n",
    "SQLITE RULES (IMPORTANT):\n",
    "- Use julianday() for date differences, NOT DATEDIFF\n",
    "- Always use table aliases to avoid ambiguous columns\n",
    "- Use ROUND() for decimal formatting\n",
    "- String comparison is case-sensitive\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "Return ONLY the SQL query. No explanation.'''\n",
    "\n",
    "print('Prompt template créé')\n",
    "print(f'Longueur: {len(IMPROVED_PROMPT)} chars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test SQL:\n",
      "SELECT COUNT(DISTINCT orders.order_id) FROM orders;\n"
     ]
    }
   ],
   "source": [
    "# Configuration LLM\n",
    "OLLAMA_URL = 'http://localhost:11434/api/generate'\n",
    "MODEL = 'mistral'\n",
    "\n",
    "def call_llm(prompt: str, temperature: float = 0.0) -> str:\n",
    "    \"\"\"Appeler le LLM via Ollama.\"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            OLLAMA_URL,\n",
    "            json={\n",
    "                'model': MODEL,\n",
    "                'prompt': prompt,\n",
    "                'stream': False,\n",
    "                'options': {'temperature': temperature, 'num_predict': 300}\n",
    "            },\n",
    "            timeout=60\n",
    "        )\n",
    "        return response.json().get('response', '').strip()\n",
    "    except Exception as e:\n",
    "        return f'ERROR: {e}'\n",
    "\n",
    "def extract_sql(response: str) -> str:\n",
    "    \"\"\"Extraire le SQL de la réponse.\"\"\"\n",
    "    sql = response.strip()\n",
    "    if '```sql' in sql:\n",
    "        sql = sql.split('```sql')[1].split('```')[0].strip()\n",
    "    elif '```' in sql:\n",
    "        sql = sql.split('```')[1].split('```')[0].strip()\n",
    "    return sql\n",
    "\n",
    "def generate_sql_improved(question: str) -> str:\n",
    "    \"\"\"Générer SQL avec prompt amélioré.\"\"\"\n",
    "    prompt = IMPROVED_PROMPT.format(schema=ENRICHED_SCHEMA, question=question)\n",
    "    response = call_llm(prompt)\n",
    "    return extract_sql(response)\n",
    "\n",
    "# Test\n",
    "test_sql = generate_sql_improved('Combien de commandes au total ?')\n",
    "print(f'Test SQL:\\n{test_sql}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fonctions d'évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Fonctions d'évaluation prêtes\n"
     ]
    }
   ],
   "source": [
    "def execute_sql(sql: str, conn) -> tuple:\n",
    "    \"\"\"Exécuter SQL et retourner (success, result).\"\"\"\n",
    "    try:\n",
    "        result = pd.read_sql(sql, conn)\n",
    "        return True, result\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "def compare_results(result1: pd.DataFrame, result2: pd.DataFrame) -> bool:\n",
    "    \"\"\"Comparer deux résultats.\"\"\"\n",
    "    try:\n",
    "        if result1.shape != result2.shape:\n",
    "            return False\n",
    "        r1 = result1.sort_values(by=result1.columns[0]).reset_index(drop=True)\n",
    "        r2 = result2.sort_values(by=result2.columns[0]).reset_index(drop=True)\n",
    "        \n",
    "        # Compare values with tolerance for floats\n",
    "        for col in r1.columns:\n",
    "            if r1[col].dtype in ['float64', 'float32']:\n",
    "                if not all(abs(r1[col] - r2[col]) < 0.01):\n",
    "                    return False\n",
    "            else:\n",
    "                if not r1[col].equals(r2[col]):\n",
    "                    # Try string comparison\n",
    "                    if not all(str(a) == str(b) for a, b in zip(r1[col], r2[col])):\n",
    "                        return False\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def evaluate_question(question_data: dict, generate_fn, conn) -> dict:\n",
    "    \"\"\"Évaluer une question.\"\"\"\n",
    "    question = question_data['question']\n",
    "    expected_sql = question_data['sql']\n",
    "    difficulty = question_data['difficulty']\n",
    "    \n",
    "    generated_sql = generate_fn(question)\n",
    "    gen_success, gen_result = execute_sql(generated_sql, conn)\n",
    "    exp_success, exp_result = execute_sql(expected_sql, conn)\n",
    "    \n",
    "    ex = False\n",
    "    if gen_success and exp_success:\n",
    "        ex = compare_results(gen_result, exp_result)\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'difficulty': difficulty,\n",
    "        'expected_sql': expected_sql,\n",
    "        'generated_sql': generated_sql,\n",
    "        'execution_success': gen_success,\n",
    "        'execution_accuracy': ex,\n",
    "        'error': None if gen_success else gen_result\n",
    "    }\n",
    "\n",
    "print('✓ Fonctions d\\'évaluation prêtes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Évaluation avec prompt amélioré"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÉVALUATION - PROMPT AMÉLIORÉ\n",
      "============================================================\n",
      "Améliorations: schéma enrichi + règles SQLite + relations\n",
      "============================================================\n",
      "[1/12] [simple] ✗ run | ✗ EX\n",
      "   Q: Combien de commandes au total ?...\n",
      "   Error: Execution failed on sql 'SQL\n",
      "SELECT COUNT(DISTINCT orders.or...\n",
      "\n",
      "[2/12] [simple] ✗ run | ✗ EX\n",
      "   Q: Combien de clients différents ?...\n",
      "   Error: Execution failed on sql 'SQL\n",
      "SELECT COUNT(DISTINCT c.custome...\n",
      "\n",
      "[3/12] [simple] ✗ run | ✗ EX\n",
      "   Q: Combien de commandes livrées ?...\n",
      "   Error: Execution failed on sql 'SQL\n",
      "SELECT COUNT(DISTINCT o.order_i...\n",
      "\n",
      "[4/12] [simple] ✓ run | ✗ EX\n",
      "   Q: Quel est le nombre de produits dans le catalogue ?...\n",
      "\n",
      "[5/12] [medium] ✓ run | ✗ EX\n",
      "   Q: Quelles sont les 5 villes avec le plus de clients ...\n",
      "\n",
      "[6/12] [medium] ✓ run | ✗ EX\n",
      "   Q: Quel est le revenue total par catégorie de produit...\n",
      "\n",
      "[7/12] [medium] ✓ run | ✗ EX\n",
      "   Q: Quelle est la note moyenne par catégorie de produi...\n",
      "\n",
      "[8/12] [medium] ✓ run | ✗ EX\n",
      "   Q: Combien de commandes par méthode de paiement ?...\n",
      "\n",
      "[9/12] [complex] ✓ run | ✗ EX\n",
      "   Q: Quels clients ont passé plus de 2 commandes ?...\n",
      "\n",
      "[10/12] [complex] ✗ run | ✗ EX\n",
      "   Q: Quel est le panier moyen par ville ?...\n",
      "   Error: Execution failed on sql 'SELECT c.customer_city, AVG(SUM(oi....\n",
      "\n",
      "[11/12] [complex] ✗ run | ✗ EX\n",
      "   Q: Quels produits ont une note moyenne inférieure à 3...\n",
      "   Error: Execution failed on sql 'SELECT product_id\n",
      "FROM order_items ...\n",
      "\n",
      "[12/12] [complex] ✓ run | ✗ EX\n",
      "   Q: Quel est le délai moyen de livraison par état ?...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('ÉVALUATION - PROMPT AMÉLIORÉ')\n",
    "print('='*60)\n",
    "print(f'Améliorations: schéma enrichi + règles SQLite + relations')\n",
    "print('='*60)\n",
    "\n",
    "results_improved = []\n",
    "for i, q in enumerate(TEST_QUESTIONS):\n",
    "    result = evaluate_question(q, generate_sql_improved, conn)\n",
    "    results_improved.append(result)\n",
    "    \n",
    "    status_ex = '✓' if result['execution_accuracy'] else '✗'\n",
    "    status_run = '✓' if result['execution_success'] else '✗'\n",
    "    \n",
    "    print(f'[{i+1}/{len(TEST_QUESTIONS)}] [{result[\"difficulty\"]}] {status_run} run | {status_ex} EX')\n",
    "    print(f'   Q: {result[\"question\"][:50]}...')\n",
    "    if not result['execution_success']:\n",
    "        print(f'   Error: {str(result[\"error\"])[:60]}...')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RÉSULTATS - PROMPT AMÉLIORÉ\n",
      "============================================================\n",
      "Execution Success: 7/12 (58.3%)\n",
      "Execution Accuracy (EX): 0/12 (0.0%)\n",
      "\n",
      "Par difficulté:\n",
      "SIMPLE   | EX: 0/4 (0%)\n",
      "MEDIUM   | EX: 0/4 (0%)\n",
      "COMPLEX  | EX: 0/4 (0%)\n",
      "\n",
      "============================================================\n",
      "COMPARAISON AVEC BASELINE\n",
      "============================================================\n",
      "Baseline EX: 25% → Amélioré EX: 0%\n",
      "Amélioration: +-25 points\n"
     ]
    }
   ],
   "source": [
    "# Résultats\n",
    "df_improved = pd.DataFrame(results_improved)\n",
    "\n",
    "total = len(df_improved)\n",
    "exec_success = df_improved['execution_success'].sum()\n",
    "ex_correct = df_improved['execution_accuracy'].sum()\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('RÉSULTATS - PROMPT AMÉLIORÉ')\n",
    "print('='*60)\n",
    "print(f'Execution Success: {exec_success}/{total} ({exec_success/total:.1%})')\n",
    "print(f'Execution Accuracy (EX): {ex_correct}/{total} ({ex_correct/total:.1%})')\n",
    "\n",
    "print('\\nPar difficulté:')\n",
    "for diff in ['simple', 'medium', 'complex']:\n",
    "    subset = df_improved[df_improved['difficulty'] == diff]\n",
    "    n = len(subset)\n",
    "    if n > 0:\n",
    "        ex = subset['execution_accuracy'].sum()\n",
    "        print(f'{diff.upper():8} | EX: {ex}/{n} ({ex/n:.0%})')\n",
    "\n",
    "# Comparaison avec baseline\n",
    "print('\\n' + '='*60)\n",
    "print('COMPARAISON AVEC BASELINE')\n",
    "print('='*60)\n",
    "print(f'Baseline EX: 25% → Amélioré EX: {ex_correct/total:.0%}')\n",
    "print(f'Amélioration: +{(ex_correct/total - 0.25)*100:.0f} points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyse des erreurs restantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreurs d'exécution: 5\n",
      "\n",
      "============================================================\n",
      "Q: Combien de commandes au total ?\n",
      "Difficulty: simple\n",
      "Generated: SQL\n",
      "SELECT COUNT(DISTINCT orders.order_id) FROM orders;...\n",
      "Error: Execution failed on sql 'SQL\n",
      "SELECT COUNT(DISTINCT orders.order_id) FROM orders;': near \"SQL\": synta\n",
      "\n",
      "============================================================\n",
      "Q: Combien de clients différents ?\n",
      "Difficulty: simple\n",
      "Generated: SQL\n",
      "SELECT COUNT(DISTINCT c.customer_id) FROM customers AS c;...\n",
      "Error: Execution failed on sql 'SQL\n",
      "SELECT COUNT(DISTINCT c.customer_id) FROM customers AS c;': near \"SQL\":\n",
      "\n",
      "============================================================\n",
      "Q: Combien de commandes livrées ?\n",
      "Difficulty: simple\n",
      "Generated: SQL\n",
      "SELECT COUNT(DISTINCT o.order_id) FROM orders AS o\n",
      "JOIN order_items AS oi ON o.order_id = oi.ord...\n",
      "Error: Execution failed on sql 'SQL\n",
      "SELECT COUNT(DISTINCT o.order_id) FROM orders AS o\n",
      "JOIN order_items AS \n",
      "\n",
      "============================================================\n",
      "Q: Quel est le panier moyen par ville ?\n",
      "Difficulty: complex\n",
      "Generated: SELECT c.customer_city, AVG(SUM(oi.price) + SUM(oi.freight_value)) as average_cart\n",
      "FROM customers AS...\n",
      "Error: Execution failed on sql 'SELECT c.customer_city, AVG(SUM(oi.price) + SUM(oi.freight_value)) as avera\n",
      "\n",
      "============================================================\n",
      "Q: Quels produits ont une note moyenne inférieure à 3 ?\n",
      "Difficulty: complex\n",
      "Generated: SELECT product_id\n",
      "FROM order_items AS oi\n",
      "JOIN reviews AS r ON oi.order_id = r.order_id\n",
      "JOIN products...\n",
      "Error: Execution failed on sql 'SELECT product_id\n",
      "FROM order_items AS oi\n",
      "JOIN reviews AS r ON oi.order_id =\n"
     ]
    }
   ],
   "source": [
    "# Erreurs d'exécution\n",
    "errors = df_improved[~df_improved['execution_success']]\n",
    "print(f'Erreurs d\\'exécution: {len(errors)}')\n",
    "\n",
    "for _, row in errors.iterrows():\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Q: {row[\"question\"]}')\n",
    "    print(f'Difficulty: {row[\"difficulty\"]}')\n",
    "    print(f'Generated: {row[\"generated_sql\"][:100]}...')\n",
    "    print(f'Error: {str(row[\"error\"])[:100]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Résultats incorrects: 7\n",
      "\n",
      "============================================================\n",
      "Q: Quel est le nombre de produits dans le catalogue ?\n",
      "\n",
      "Expected: SELECT COUNT(*) as total FROM products...\n",
      "\n",
      "Generated: SELECT COUNT(DISTINCT products.product_id) FROM products;...\n",
      "\n",
      "Expected result: {'total': {0: 32951}}\n",
      "Generated result: {'COUNT(DISTINCT products.product_id)': {0: 32951}}\n",
      "\n",
      "============================================================\n",
      "Q: Quelles sont les 5 villes avec le plus de clients ?\n",
      "\n",
      "Expected: SELECT customer_city, COUNT(*) as num_customers \n",
      "                  FROM customer...\n",
      "\n",
      "Generated: SELECT customer_city, COUNT(*) as number_of_customers\n",
      "FROM customers c\n",
      "JOIN orde...\n",
      "\n",
      "Expected result: {'customer_city': {0: 'sao paulo', 1: 'rio de janeiro'}, 'num_customers': {0: 15540, 1: 6882}}\n",
      "Generated result: {'customer_city': {0: 'sao paulo', 1: 'rio de janeiro'}, 'number_of_customers': {0: 15540, 1: 6882}}\n",
      "\n",
      "============================================================\n",
      "Q: Quel est le revenue total par catégorie de produit ?\n",
      "\n",
      "Expected: SELECT p.product_category, SUM(oi.price) as revenue\n",
      "                  FROM order...\n",
      "\n",
      "Generated: SELECT\n",
      "    p.product_category,\n",
      "    SUM(oip.price + oip.freight_value) AS total_r...\n",
      "\n",
      "Expected result: {'product_category': {0: 'beleza_saude', 1: 'relogios_presentes'}, 'revenue': {0: 1258681.34, 1: 1205005.68}}\n",
      "Generated result: {'product_category': {0: nan, 1: 'agro_industria_e_comercio'}, 'total_revenue': {0: 207792.6, 1: 78374.07}}\n",
      "\n",
      "============================================================\n",
      "Q: Quelle est la note moyenne par catégorie de produit ?\n",
      "\n",
      "Expected: SELECT p.product_category, ROUND(AVG(r.review_score), 2) as avg_score\n",
      "          ...\n",
      "\n",
      "Generated: SELECT\n",
      "    p.product_category,\n",
      "    AVG(r.review_score) AS average_score\n",
      "FROM\n",
      "   ...\n",
      "\n",
      "Expected result: {'product_category': {0: 'cds_dvds_musicais', 1: 'fashion_roupa_infanto_juvenil'}, 'avg_score': {0: 4.64, 1: 4.5}}\n",
      "Generated result: {'product_category': {0: nan, 1: 'agro_industria_e_comercio'}, 'average_score': {0: 3.8410513141426783, 1: 4.0}}\n",
      "\n",
      "============================================================\n",
      "Q: Combien de commandes par méthode de paiement ?\n",
      "\n",
      "Expected: SELECT payment_type, COUNT(DISTINCT order_id) as num_orders\n",
      "                  FR...\n",
      "\n",
      "Generated: SELECT payment_type, COUNT(orders.order_id) as order_count\n",
      "FROM payments\n",
      "JOIN or...\n",
      "\n",
      "Expected result: {'payment_type': {0: 'credit_card', 1: 'boleto'}, 'num_orders': {0: 76505, 1: 19784}}\n",
      "Generated result: {'payment_type': {0: 'boleto', 1: 'credit_card'}, 'order_count': {0: 19784, 1: 76795}}\n",
      "\n",
      "============================================================\n",
      "Q: Quels clients ont passé plus de 2 commandes ?\n",
      "\n",
      "Expected: SELECT c.customer_id, c.customer_city, COUNT(o.order_id) as num_orders\n",
      "         ...\n",
      "\n",
      "Generated: SELECT customer_id\n",
      "FROM customers c\n",
      "WHERE (\n",
      "    SELECT COUNT(*) FROM orders o WH...\n",
      "\n",
      "Expected result: {'customer_id': {}, 'customer_city': {}, 'num_orders': {}}\n",
      "Generated result: {'customer_id': {}}\n",
      "\n",
      "============================================================\n",
      "Q: Quel est le délai moyen de livraison par état ?\n",
      "\n",
      "Expected: SELECT c.customer_state,\n",
      "                         ROUND(AVG(julianday(o.order_de...\n",
      "\n",
      "Generated: SELECT\n",
      "  customer_state,\n",
      "  AVG(JULIANDAY('order_delivered_timestamp') - JULIANDA...\n",
      "\n",
      "Expected result: {'customer_state': {0: 'SP', 1: 'MG'}, 'avg_delivery_days': {0: 8.8, 1: 12.0}}\n",
      "Generated result: {'customer_state': {0: 'AC', 1: 'AL'}, 'avg_delivery_delay': {0: None, 1: None}}\n"
     ]
    }
   ],
   "source": [
    "# Mauvais résultats\n",
    "wrong = df_improved[df_improved['execution_success'] & ~df_improved['execution_accuracy']]\n",
    "print(f'\\nRésultats incorrects: {len(wrong)}')\n",
    "\n",
    "for _, row in wrong.iterrows():\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Q: {row[\"question\"]}')\n",
    "    print(f'\\nExpected: {row[\"expected_sql\"][:80]}...')\n",
    "    print(f'\\nGenerated: {row[\"generated_sql\"][:80]}...')\n",
    "    \n",
    "    # Montrer la différence de résultats\n",
    "    _, exp_res = execute_sql(row['expected_sql'], conn)\n",
    "    _, gen_res = execute_sql(row['generated_sql'], conn)\n",
    "    print(f'\\nExpected result: {exp_res.head(2).to_dict()}')\n",
    "    print(f'Generated result: {gen_res.head(2).to_dict()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sauvegarder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Résultats sauvegardés\n"
     ]
    }
   ],
   "source": [
    "improved_results = {\n",
    "    'model': MODEL,\n",
    "    'prompt_type': 'improved (enriched schema + SQLite rules)',\n",
    "    'total_questions': total,\n",
    "    'metrics': {\n",
    "        'execution_success': exec_success / total,\n",
    "        'execution_accuracy': ex_correct / total\n",
    "    },\n",
    "    'by_difficulty': {},\n",
    "    'comparison': {\n",
    "        'baseline_ex': 0.25,\n",
    "        'improved_ex': ex_correct / total,\n",
    "        'improvement': (ex_correct / total) - 0.25\n",
    "    }\n",
    "}\n",
    "\n",
    "for diff in ['simple', 'medium', 'complex']:\n",
    "    subset = df_improved[df_improved['difficulty'] == diff]\n",
    "    n = len(subset)\n",
    "    if n > 0:\n",
    "        improved_results['by_difficulty'][diff] = {\n",
    "            'total': n,\n",
    "            'execution_accuracy': subset['execution_accuracy'].sum() / n\n",
    "        }\n",
    "\n",
    "with open(DATA_DIR / 'results' / 'improved_results.json', 'w') as f:\n",
    "    json.dump(improved_results, f, indent=2)\n",
    "\n",
    "print(f'✓ Résultats sauvegardés')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résumé\n",
    "\n",
    "| Métrique | Baseline | Amélioré |\n",
    "|----------|----------|----------|\n",
    "| Exec Success | 75% | ?% |\n",
    "| Exec Accuracy | 25% | ?% |\n",
    "\n",
    "### Prochaine étape :\n",
    "**Notebook 04** : Ajouter des exemples few-shot pour encore améliorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()\n",
    "print('Connexion fermée.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
